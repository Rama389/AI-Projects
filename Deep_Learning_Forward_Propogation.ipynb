{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt0gPpVU8Bi4YQResezNXA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rama389/AI-Projects/blob/main/Deep_Learning_Forward_Propogation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning"
      ],
      "metadata": {
        "id": "AlRWVumm442v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation"
      ],
      "metadata": {
        "id": "8Bka3wTD48Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procedure for calculation of Xavier weights\n",
        "\n",
        "Input to hidden:\n",
        "\n",
        "inputs = 2\n",
        "\n",
        "hidden = 2 (neurons)\n",
        "\n",
        "output = 1\n",
        "\n",
        "Input1 (i1) = 2\n",
        "\n",
        "Input2 (i2) = 4\n",
        "\n",
        "Linear Activation function\n"
      ],
      "metadata": {
        "id": "FMSezmmr5xSH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9GnXlNv4tVC",
        "outputId": "6100881b-79a6-42c1-c9af-9cd131ed776f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Xavier Initialized Weights ===\n",
            "W1, W2, W3, W4 (input -> hidden):\n",
            "[[-0.30731269  1.10402007]\n",
            " [ 0.56826678  0.24166295]]\n",
            "W5, W6 (hidden -> output):\n",
            "[-0.97292621 -0.97299443]\n",
            "\n",
            "=== Forward Propagation ===\n",
            "h1 = 1.6584\n",
            "h2 = 3.1747\n",
            "out (prediction) = -4.7025\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ----------------------------\n",
        "# Xavier Initialization Helper\n",
        "# ----------------------------\n",
        "def xavier_init(fan_in, fan_out):\n",
        "    limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "    return np.random.uniform(-limit, limit, size=(fan_in, fan_out))\n",
        "\n",
        "# ----------------------------\n",
        "# Network Setup: 2 -> 2 -> 1\n",
        "# ----------------------------\n",
        "np.random.seed(42)  # for reproducibility\n",
        "\n",
        "# Inputs\n",
        "i1, i2 = 2.0, 4.0\n",
        "x = np.array([i1, i2])  # shape (2,)\n",
        "\n",
        "# Xavier Initialization\n",
        "W_input_hidden = xavier_init(2, 2)   # shape (2,2) -> W1..W4\n",
        "W_hidden_output = xavier_init(2, 1)  # shape (2,1) -> W5..W6\n",
        "\n",
        "# Report initialized weights\n",
        "print(\"=== Xavier Initialized Weights ===\")\n",
        "print(\"W1, W2, W3, W4 (input -> hidden):\")\n",
        "print(W_input_hidden)\n",
        "print(\"W5, W6 (hidden -> output):\")\n",
        "print(W_hidden_output.flatten())\n",
        "\n",
        "# ----------------------------\n",
        "# Forward Propagation (linear hidden)\n",
        "# ----------------------------\n",
        "# Hidden layer (linear activation, so no tanh/sigmoid)\n",
        "z_h = np.dot(x, W_input_hidden)   # shape (2,) -> [h1, h2]\n",
        "h1, h2 = z_h\n",
        "\n",
        "# Output layer (linear)\n",
        "z_out = np.dot(z_h, W_hidden_output).item()\n",
        "\n",
        "# Report forward pass\n",
        "print(\"\\n=== Forward Propagation ===\")\n",
        "print(f\"h1 = {h1:.4f}\")\n",
        "print(f\"h2 = {h2:.4f}\")\n",
        "print(f\"out (prediction) = {z_out:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weights are;\n",
        "w1 = -0.3, w2 = -1.1, w3 = 0.56, w4 = 0.24, w5 = -0.97, w6 = 0.24\n",
        "\n",
        "Output = -4.7025"
      ],
      "metadata": {
        "id": "amTIhjDF6so_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question - 4"
      ],
      "metadata": {
        "id": "zC4yi3TQ7dHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU Activation Function"
      ],
      "metadata": {
        "id": "EpcDjxYPoFwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# given\n",
        "W1, W2, W3, W4 = -0.30731343, 1.10402055, 0.56826769, 0.24166314\n",
        "W5, W6 = -0.97292573, -0.97299407\n",
        "i1, i2 = 2.0, 4.0\n",
        "\n",
        "# pre-activations\n",
        "h1 = W1*i1 + W2*i2\n",
        "h2 = W3*i1 + W4*i2\n",
        "\n",
        "# ReLU\n",
        "h1_relu = max(0.0, h1)\n",
        "h2_relu = max(0.0, h2)\n",
        "\n",
        "# output\n",
        "out = W5*h1_relu + W6*h2_relu\n",
        "\n",
        "print(\"h1:\", h1)\n",
        "print(\"h2:\", h2)\n",
        "print(\"h1 (ReLU):\", h1_relu)\n",
        "print(\"h2 (ReLU):\", h2_relu)\n",
        "print(\"out:\", out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4vsR2Bb7D7I",
        "outputId": "d8a04995-9146-4add-bc15-e0f1788725f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h1: 3.80145534\n",
            "h2: 2.1031879399999998\n",
            "h1 (ReLU): 3.80145534\n",
            "h2 (ReLU): 2.1031879399999998\n",
            "out: -5.744923105447414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ReLU activation function the output value is -5.7449"
      ],
      "metadata": {
        "id": "09f3bqCeoWKi"
      }
    }
  ]
}